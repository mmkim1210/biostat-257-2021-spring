{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biostat 257 Homework 4\n",
    "\n",
    "**Due May 28 @ 11:59PM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julia Version 1.6.0\n",
      "Commit f9720dc2eb (2021-03-24 12:55 UTC)\n",
      "Platform Info:\n",
      "  OS: macOS (x86_64-apple-darwin19.6.0)\n",
      "  CPU: Intel(R) Core(TM) i9-9880H CPU @ 2.30GHz\n",
      "  WORD_SIZE: 64\n",
      "  LIBM: libopenlibm\n",
      "  LLVM: libLLVM-11.0.1 (ORCJIT, skylake)\n"
     ]
    }
   ],
   "source": [
    "versioninfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we continue with the linear mixed effects model (LMM) considered in HW2\n",
    "$$\n",
    "    \\mathbf{Y}_i = \\mathbf{X}_i \\boldsymbol{\\beta} + \\mathbf{Z}_i \\boldsymbol{\\gamma}_i + \\boldsymbol{\\epsilon}_i, \\quad i=1,\\ldots,n,\n",
    "$$\n",
    "where   \n",
    "- $\\mathbf{Y}_i \\in \\mathbb{R}^{n_i}$ is the response vector of $i$-th individual,  \n",
    "- $\\mathbf{X}_i \\in \\mathbb{R}^{n_i \\times p}$ is the fixed effects predictor matrix of $i$-th individual,  \n",
    "- $\\mathbf{Z}_i \\in \\mathbb{R}^{n_i \\times q}$ is the random effects predictor matrix of $i$-th individual,  \n",
    "- $\\boldsymbol{\\epsilon}_i \\in \\mathbb{R}^{n_i}$ are multivariate normal $N(\\mathbf{0}_{n_i},\\sigma^2 \\mathbf{I}_{n_i})$,  \n",
    "- $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$ are fixed effects, and  \n",
    "- $\\boldsymbol{\\gamma}_i \\in \\mathbb{R}^q$ are random effects assumed to be $N(\\mathbf{0}_q, \\boldsymbol{\\Sigma}_{q \\times q}$) independent of $\\boldsymbol{\\epsilon}_i$.\n",
    "\n",
    "The log-likelihood of the $i$-th datum $(\\mathbf{y}_i, \\mathbf{X}_i, \\mathbf{Z}_i)$ is \n",
    "$$\n",
    "    \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2) = - \\frac{n_i}{2} \\log (2\\pi) - \\frac{1}{2} \\log \\det \\boldsymbol{\\Omega}_i - \\frac{1}{2} (\\mathbf{y} - \\mathbf{X}_i \\boldsymbol{\\beta})^T \\boldsymbol{\\Omega}_i^{-1} (\\mathbf{y} - \\mathbf{X}_i \\boldsymbol{\\beta}),\n",
    "$$\n",
    "where\n",
    "$$\n",
    "    \\boldsymbol{\\Omega}_i = \\sigma^2 \\mathbf{I}_{n_i} + \\mathbf{Z}_i \\boldsymbol{\\Sigma} \\mathbf{Z}_i^T = \\sigma^2 \\mathbf{I}_{n_i} + \\mathbf{Z}_i \\mathbf{L} \\mathbf{L}^T \\mathbf{Z}_i^T,\n",
    "$$\n",
    "Because the variance component parameter $\\boldsymbol{\\Sigma}$ has to be positive semidefinite, we use its Cholesky factor $\\mathbf{L}$ as optimization variable. \n",
    "\n",
    "Given $m$ independent data points $(\\mathbf{y}_i, \\mathbf{X}_i, \\mathbf{Z}_i)$, $i=1,\\ldots,m$, we seek the maximum likelihood estimate (MLE) by maximizing the log-likelihood\n",
    "$$\n",
    "\\ell(\\boldsymbol{\\beta}, \\boldsymbol{\\Sigma}, \\sigma^2) = \\sum_{i=1}^m \\ell_i(\\boldsymbol{\\beta}, \\boldsymbol{\\Sigma}, \\sigma^2).\n",
    "$$\n",
    "In this assignment, we use the nonlinear programming (NLP) approach for optimization. In HW5, we will derive a EM (expectation-maximization) algorithm for the same problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load necessary packages; make sure to install them first\n",
    "using BenchmarkTools, CSV, DataFrames, DelimitedFiles, Distributions\n",
    "using Ipopt, LinearAlgebra, MathProgBase, MixedModels, NLopt\n",
    "using Random, RCall, Revise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. (Optional, 30 bonus pts) Derivatives\n",
    "\n",
    "NLP optimization solvers expect users to provide at least a function for evaluating objective value. If users can provide further information such as gradient and Hessian, the NLP solvers will be more stable and converge faster. Automatic differentiation tools are becoming more powerful but cannot apply to all problems yet.\n",
    "\n",
    "1. Show that the gradient of $\\ell_i$ is\n",
    "\\begin{eqnarray*}\n",
    "\\nabla_{\\boldsymbol{\\beta}} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2) &=& \\mathbf{X}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i, \\\\\n",
    "\\nabla_{\\sigma^2} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2) &=& - \\frac{1}{2} \\text{tr} (\\boldsymbol{\\Omega}_i^{-1}) + \\frac{1}{2} \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-2} \\mathbf{r}_i, \\\\\n",
    "\\frac{\\partial}{\\partial \\mathbf{L}} \\ell_i(\\boldsymbol{\\beta}, \\mathbf{L}, \\sigma^2) &=& - \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L} + \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L},\n",
    "\\end{eqnarray*}\n",
    "where $\\mathbf{r}_i = \\mathbf{y}_i - \\mathbf{X}_i \\boldsymbol{\\beta}$. \n",
    "\n",
    "2. Derive the observed information matrix and the expected (Fisher) information matrix.\n",
    "\n",
    "If you need a refresher on multivariate calculus, my [Biostat 216 lecture notes](https://ucla-biostat216-2019fall.github.io/slides/16-matrixcalc/16-matrixcalc.html) may be helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $\\nabla_{\\boldsymbol{\\beta}} \\ell_i$,\n",
    "\\begin{eqnarray*}\n",
    "\\text{d} \\ell_i &=& \\frac{1}{2} (\\mathbf{X}_i \\text{d} \\boldsymbol{\\beta})^T \\boldsymbol{\\Omega}_i^{-1} (\\mathbf{y} - \\mathbf{X}_i \\boldsymbol{\\beta}) + \\frac{1}{2} (\\mathbf{y} - \\mathbf{X}_i \\boldsymbol{\\beta})^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{X}_i \\text{d} \\boldsymbol{\\beta}\\\\\n",
    "&=& (\\mathbf{y} - \\mathbf{X}_i \\boldsymbol{\\beta})^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{X}_i \\text{d} \\boldsymbol{\\beta} = \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{X}_i \\text{d} \\boldsymbol{\\beta} \\\\\n",
    "\\therefore \\text{D}_{\\boldsymbol{\\beta}} \\ell_i &=& \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{X}_i  \\\\\n",
    "\\nabla_{\\boldsymbol{\\beta}} \\ell_i &=& (\\text{D}_{\\boldsymbol{\\beta}} \\ell_i)^T =\\mathbf{X}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $\\nabla_{\\sigma^2} \\ell_i$,\n",
    "\\begin{eqnarray*}\n",
    "\\text{d} \\ell_i &=& - \\frac{1}{2} \\text{tr} (\\boldsymbol{\\Omega}_i^{-1} \\text{d}\\boldsymbol{\\Omega}_i) + \\frac{1}{2} \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\text{d}\\boldsymbol{\\Omega}_i \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i \\\\\n",
    "&=& - \\frac{1}{2} \\text{tr} (\\boldsymbol{\\Omega}_i^{-1} \\text{d}\\sigma^2) + \\frac{1}{2} \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-2} \\mathbf{r}_i \\text{d}\\sigma^2 \\quad (\\because \\text{d}\\boldsymbol{\\Omega}_i = \\text{d} \\sigma^2 \\mathbf I) \\\\\n",
    "\\therefore \\text{D}_{\\sigma^2} \\ell_i &=& - \\frac{1}{2} \\text{tr} (\\boldsymbol{\\Omega}_i^{-1}) + \\frac{1}{2} \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-2} \\mathbf{r}_i = \\nabla_{\\sigma^2} \\ell_i \\\\\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $\\frac{\\partial}{\\partial \\mathbf{L}} \\ell_i$,\n",
    "\\begin{eqnarray*}\n",
    "\\text{d} \\ell_i &=& - \\frac{1}{2} \\text{tr} (\\boldsymbol{\\Omega}_i^{-1} \\text{d}\\boldsymbol{\\Omega}_i) + \\frac{1}{2} \\text{tr} (\\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\text{d}\\boldsymbol{\\Omega}_i \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i) \\\\\n",
    "\\text{d} \\boldsymbol{\\Omega}_i &=& \\mathbf{Z}_i \\text{d} \\mathbf{L} \\mathbf{L}^T \\mathbf{Z}_i^T + \\mathbf{Z}_i \\mathbf{L} \\text{d} \\mathbf{L}^T \\mathbf{Z}_i^T\n",
    "\\end{eqnarray*}\n",
    "\n",
    "By substituting and rearranging terms inside trace, it is straightforward to show that\n",
    "\\begin{eqnarray*}\n",
    "\\text{d} \\ell_i &=& \\text{tr} (- \\mathbf{L}^T \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\text{d} \\mathbf{L} + \\mathbf{L}^T \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\text{d} \\mathbf{L}) \\\\\n",
    "\\therefore \\frac{\\partial}{\\partial \\mathbf{L}} \\ell_i &=& - \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L} + \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L}\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $\\text{D}_{\\text{vech} \\mathbf{L}} \\ell_i$, it can be also shown that\n",
    "\\begin{eqnarray*}\n",
    "\\text{d} \\ell_i &=& \\text{vec} (- \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L} + \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L})^T \\text{dvec} \\mathbf{L} \\\\\n",
    "&=& \\text{vec} (- \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L} + \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L})^T \\mathbf{D}_q \\text{dvech} \\mathbf{L}\n",
    "\\end{eqnarray*}\n",
    "where $\\mathbf{D}_q$ is the triangular duplication matrix such that\n",
    "\\begin{eqnarray*}\n",
    "\\text{D}_{\\text{vech} \\mathbf{L}} \\ell_i &=& \\text{vec} (- \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L} + \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L})^T \\mathbf{D}_q.\n",
    "\\end{eqnarray*}\n",
    "Alternatively,  \n",
    "\\begin{eqnarray*}\n",
    "\\text{d} \\ell_i &=& \\text{vec} (- \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L})^T \\text{dvec} \\mathbf{L} + \\frac{1}{2}\\text{vec} (\\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\text{d} \\mathbf{L} \\mathbf{L}^T \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i) + \\frac{1}{2}\\text{vec} (\\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L} \\text{d} \\mathbf{L}^T \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i) \\\\\n",
    "&=& \\text{vec} (- \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L})^T \\text{dvec} \\mathbf{L} + \\frac{1}{2} (\\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L} \\otimes \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i) \\text{dvec} \\mathbf{L} + \\frac{1}{2} (\\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\otimes \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L}) \\text{dvec} \\mathbf{L}^T \\\\\n",
    "&=& \\text{vec} (- \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L})^T \\text{dvec} \\mathbf{L} + \\frac{1}{2} (\\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L} \\otimes \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i) \\text{dvec} \\mathbf{L} + \\frac{1}{2} (\\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\otimes \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L}) \\mathbf{K}_{qq}\\text{dvec} \\mathbf{L}\n",
    "\\end{eqnarray*}\n",
    "where $\\mathbf{K}_{qq}$ is the commutation matrix such that\n",
    "\\begin{eqnarray*}\n",
    "\\text{D}_{\\text{vech} \\mathbf{L}} \\ell_i &=& \\text{vec} (- \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L})^T \\mathbf{D}_q + \\frac{1}{2} (\\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L} \\otimes \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i) \\mathbf{D}_q + \\frac{1}{2} (\\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\otimes \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L}) \\mathbf{K}_{qq} \\mathbf{D}_q.\n",
    "\\end{eqnarray*}\n",
    "The above expressions for $\\text{D}_{\\text{vech} \\mathbf{L}} \\ell_i$ are equivalent, since $\\text{vec} (\\boldsymbol{a}\\boldsymbol{b}^T) = \\boldsymbol{b} \\otimes \\boldsymbol{a}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For observed and expected Fisher information matrices,\n",
    "\\begin{eqnarray*}\n",
    "\\text{d} (\\nabla_{\\boldsymbol{\\beta}} \\ell_i) &=& -\\mathbf{X}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{X}_i \\text{d} \\boldsymbol{\\beta} \\\\\n",
    "\\therefore - \\nabla^2_{\\boldsymbol{\\beta}} \\ell_i &=& \\mathbf{X}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{X}_i \\rightarrow \\mathbb{E}[- \\nabla^2_{\\boldsymbol{\\beta}} \\ell_i] = \\mathbf{X}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{X}_i\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray*}\n",
    "\\text{d}(\\frac{\\partial}{\\partial \\boldsymbol{\\beta}} \\ell_i) &=& - \\mathbf{X}_i^T \\boldsymbol{\\Omega}_i^{-1} \\text{d} \\boldsymbol{\\Omega}_i \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i = - \\mathbf{X}_i^T \\boldsymbol{\\Omega}_i^{-2} \\mathbf{r}_i \\text{d} \\sigma^2\\\\\n",
    "\\therefore \\frac{\\partial^2}{\\partial \\sigma^2 \\partial \\boldsymbol{\\beta}} \\ell_i &=&  - \\mathbf{X}_i^T \\boldsymbol{ \\Omega}_i^{-2} \\mathbf{r}_i \\rightarrow \\mathbb{E}[- \\frac{\\partial^2}{\\partial \\sigma^2 \\partial \\boldsymbol{\\beta}} \\ell_i] = \\mathbf{0}_{p \\times 1}\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray*}\n",
    "\\text{d}(\\frac{\\partial}{\\partial \\boldsymbol{\\beta}} \\ell_i) &=& - \\mathbf{X}_i^T \\boldsymbol{\\Omega}_i^{-1} \\text{d} \\boldsymbol{\\Omega}_i \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i \\\\\n",
    "&=& - \\mathbf{X}_i^T \\boldsymbol{\\Omega}_i^{-1} (\\mathbf{Z}_i \\text{d} \\mathbf{L} \\mathbf{L}^T \\mathbf{Z}_i^T + \\mathbf{Z}_i \\mathbf{L} \\text{d} \\mathbf{L}^T \\mathbf{Z}_i^T) \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i \\\\\n",
    "&=& - (\\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L} \\otimes \\mathbf{X}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i ) \\text{dvec} \\mathbf{L} - (\\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\otimes \\mathbf{X}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L}) \\text{dvec} \\mathbf{L}^T  \\\\\n",
    "&=& - (\\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L} \\otimes \\mathbf{X}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i ) \\text{dvec} \\mathbf{L} - (\\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\otimes \\mathbf{X}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L}) \\mathbf{K}_{qq}\\text{dvec} \\mathbf{L} \\\\\n",
    "\\therefore \\frac{\\partial^2}{\\partial (\\text{vech} \\mathbf{L})^T \\partial \\boldsymbol{\\beta}} \\ell_i &=&  - (\\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L} \\otimes \\mathbf{X}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i ) \\mathbf{D}_q - (\\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\otimes \\mathbf{X}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L}) \\mathbf{K}_{qq} \\mathbf{D}_q \\\\ \n",
    "&\\rightarrow& \\mathbb{E}[- \\frac{\\partial^2}{\\partial (\\text{vech} \\mathbf{L})^T \\partial \\boldsymbol{\\beta}} \\ell_i] = \\mathbf{0}_{p \\times \\frac{q(q+1)}{2}}\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray*}\n",
    "\\text{d} (\\nabla_{\\sigma^2} \\ell_i) &=& \\frac{1}{2} \\text{tr} (\\boldsymbol{\\Omega}_i^{-1} \\text{d} \\boldsymbol{\\Omega}_i \\boldsymbol{\\Omega}_i^{-1}) - \\frac{1}{2} \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\text{d} \\boldsymbol{\\Omega}_i \\boldsymbol{\\Omega}_i^{-2} \\mathbf{r}_i - \\frac{1}{2} \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-2} \\text{d} \\boldsymbol{\\Omega}_i \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i  \\\\\n",
    "&=& \\frac{1}{2} \\text{tr} (\\boldsymbol{\\Omega}_i^{-2} \\text{d} \\sigma^2) - \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-3} \\mathbf{r}_i \\text{d} \\sigma^2 \\quad (\\because \\text{d}\\boldsymbol{\\Omega}_i = \\text{d} \\sigma^2 \\mathbf I) \\\\\n",
    "\\therefore \\nabla^2_{\\boldsymbol{\\sigma}^2} \\ell_i &=& \\frac{\\partial^2}{(\\partial \\sigma^2)^2} \\ell_i = \\frac{1}{2} \\text{tr} (\\boldsymbol{\\Omega}_i^{-2}) - \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-3} \\mathbf{r}_i \\rightarrow \\mathbb{E} [- \\nabla^2_{\\boldsymbol{\\sigma}^2} \\ell_i] = - \\frac{1}{2} \\text{tr} (\\boldsymbol{\\Omega}_i^{-2}) + \\text{tr} (\\boldsymbol{\\Omega}_i^{-2}) = \\frac{1}{2} \\text{tr} (\\boldsymbol{\\Omega}_i^{-2})\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray*}\n",
    "\\text{d} (\\nabla_{\\sigma^2} \\ell_i) &=& \\frac{1}{2} \\text{tr} (\\boldsymbol{\\Omega}_i^{-1} \\text{d} \\boldsymbol{\\Omega}_i \\boldsymbol{\\Omega}_i^{-1}) - \\frac{1}{2} \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\text{d} \\boldsymbol{\\Omega}_i \\boldsymbol{\\Omega}_i^{-2} \\mathbf{r}_i - \\frac{1}{2} \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-2} \\text{d} \\boldsymbol{\\Omega}_i \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i  \\\\\n",
    "&=& \\text{tr} (\\mathbf{L}^T \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-2} \\mathbf{Z}_i \\text{d} \\mathbf{L} - \\mathbf{L}^T \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-2} \\mathbf{r}_i \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\text{d} \\mathbf{L} - \\mathbf{L}^T \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-2} \\mathbf{Z}_i \\text{d} \\mathbf{L}) \\\\\n",
    "&=& \\text{vec} (\\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-2} \\mathbf{Z}_i \\mathbf{L} - \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-2} \\mathbf{Z}_i \\mathbf{L} - \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-2} \\mathbf{r}_i \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L})^T \\mathbf{D}_q \\text{dvech} \\mathbf{L} \\\\\n",
    "\\therefore \\frac{\\partial^2}{\\partial (\\text{vech} \\mathbf{L})^T \\partial \\sigma^2} \\ell_i &=& \\text{vec} (\\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-2} \\mathbf{Z}_i \\mathbf{L} - \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-2} \\mathbf{Z}_i \\mathbf{L} - \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-2} \\mathbf{r}_i \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L})^T \\mathbf{D}_q \\\\ \n",
    "&\\rightarrow& \\mathbb{E}[- \\frac{\\partial^2}{\\partial (\\text{vech} \\mathbf{L})^T \\partial \\sigma^2} \\ell_i] = \\text{vec}(\\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-2} \\mathbf{Z}_i \\mathbf{L})^T \\mathbf{D}_q \\quad (\\because \\mathbb{E}[\\mathbf{r}_i \\mathbf{r}_i^T] = \\boldsymbol{\\Omega}_i)\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray*}\n",
    "\\text{d} (\\text{D}_{\\text{vech} \\mathbf{L}} \\ell_i)^T &=& \\mathbf{D}_q^T \\text{vec} (\\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\text{d} \\boldsymbol{\\Omega}_i \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L} + \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\text{d} \\mathbf{L} \\\\\n",
    "&& - \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\text{d} \\boldsymbol{\\Omega}_i \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L} - \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\text{d} \\boldsymbol{\\Omega}_i \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L} + \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\text{d} \\mathbf{L}) \\\\\n",
    "&=& \\mathbf{D}_q^T [(\\mathbf{L}^T \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L} \\otimes \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i) \\text{dvec} \\mathbf{L} + (\\mathbf{L}^T \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\otimes \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L}) \\mathbf{K}_{qq} \\text{dvec} \\mathbf{L} \\\\\n",
    "&& - (\\mathbf{I}_q \\otimes \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i) \\text{dvec} \\mathbf{L} - (\\mathbf{L}^T \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L} \\otimes \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i) \\text{dvec} \\mathbf{L} \\\\\n",
    "&& - (\\mathbf{L}^T \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\otimes \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L}) \\mathbf{K}_{qq} \\text{dvec} \\mathbf{L} \\\\\n",
    "&& - (\\mathbf{L}^T \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L} \\otimes \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i) \\text{dvec} \\mathbf{L} \\\\\n",
    "&& - (\\mathbf{L}^T \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\otimes \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L}) \\mathbf{K}_{qq} \\text{dvec} \\mathbf{L} \\\\\n",
    "&& + (\\mathbf{I}_q \\otimes \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i) \\text{dvec} \\mathbf{L}] \\\\\n",
    "\\therefore \\frac{\\partial^2}{\\partial (\\text{vech} \\mathbf{L})^T \\partial \\text{vech} \\mathbf{L}} \\ell_i &=& \\mathbf{D}_q^T [(\\mathbf{L}^T \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L} \\otimes \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i) + (\\mathbf{L}^T \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\otimes \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L}) \\mathbf{K}_{qq} \\\\\n",
    "&& - (\\mathbf{I}_q \\otimes \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i) - (\\mathbf{L}^T \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L} \\otimes \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i) \\\\\n",
    "&& - (\\mathbf{L}^T \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\otimes \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L}) \\mathbf{K}_{qq} \\\\\n",
    "&& - (\\mathbf{L}^T \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L} \\otimes \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i) \\\\\n",
    "&& - (\\mathbf{L}^T \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\otimes \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L}) \\mathbf{K}_{qq} \\\\\n",
    "&& + (\\mathbf{I}_q \\otimes \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{r}_i \\mathbf{r}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i)] \\mathbf{D}_q \\\\\n",
    "\\rightarrow \\mathbb{E}[- \\frac{\\partial^2}{\\partial (\\text{vech} \\mathbf{L})^T \\partial \\text{vech} \\mathbf{L}} \\ell_i] &=& \\mathbf{D}_q^T [\\mathbf{L}^T \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L} \\otimes \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i + (\\mathbf{L}^T \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\otimes \\mathbf{Z}_i^T \\boldsymbol{\\Omega}_i^{-1} \\mathbf{Z}_i \\mathbf{L}) \\mathbf{K}_{qq}] \\mathbf{D}_q \n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. (20 pts) Objective and gradient evaluator for a single datum\n",
    "\n",
    "We expand the code from HW2 to evaluate both objective and gradient. I provide my code for HW2 below as a starting point. You do _not_ have to use this code. If your come up faster code, that's even better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logl!"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a type that holds an LMM datum\n",
    "struct LmmObs{T <: AbstractFloat}\n",
    "    # data\n",
    "    y          :: Vector{T}\n",
    "    X          :: Matrix{T}\n",
    "    Z          :: Matrix{T}\n",
    "    # arrays for holding gradient\n",
    "    ∇β         :: Vector{T}\n",
    "    ∇σ²        :: Vector{T}\n",
    "    ∇L         :: Matrix{T}    \n",
    "    # working arrays\n",
    "    # TODO: whatever intermediate arrays you may want to pre-allocate\n",
    "    yty        :: T\n",
    "    xty        :: Vector{T}\n",
    "    zty        :: Vector{T}\n",
    "    storage_p  :: Vector{T}\n",
    "    storage_q  :: Vector{T}\n",
    "    xtx        :: Matrix{T}\n",
    "    ztx        :: Matrix{T}\n",
    "    ztz        :: Matrix{T}\n",
    "    storage_qq :: Matrix{T}\n",
    "    LM⁻¹LᵗZᵗZ     :: Matrix{T}\n",
    "    ZᵗZLM⁻¹LᵗZᵗr  :: Vector{T}\n",
    "    Zᵗr           :: Vector{T}\n",
    "    ZᵗΩ⁻¹r        :: Vector{T}\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    LmmObs(y::Vector, X::Matrix, Z::Matrix)\n",
    "\n",
    "Create an LMM datum of type `LmmObs`.\n",
    "\"\"\"\n",
    "function LmmObs(\n",
    "        y::Vector{T}, \n",
    "        X::Matrix{T}, \n",
    "        Z::Matrix{T}\n",
    "    ) where T <: AbstractFloat\n",
    "    n, p, q    = size(X, 1), size(X, 2), size(Z, 2)    \n",
    "    ∇β         = Vector{T}(undef, p)\n",
    "    ∇σ²        = Vector{T}(undef, 1)\n",
    "    ∇L         = Matrix{T}(undef, q, q)    \n",
    "    yty        = abs2(norm(y))\n",
    "    xty        = transpose(X) * y\n",
    "    zty        = transpose(Z) * y    \n",
    "    storage_p  = Vector{T}(undef, p)\n",
    "    storage_q  = Vector{T}(undef, q)\n",
    "    xtx        = transpose(X) * X\n",
    "    ztx        = transpose(Z) * X\n",
    "    ztz        = transpose(Z) * Z\n",
    "    storage_qq = similar(ztz)\n",
    "    LM⁻¹LᵗZᵗZ     = Matrix{T}(undef, q, q)\n",
    "    ZᵗZLM⁻¹LᵗZᵗr  = Vector{T}(undef, q)\n",
    "    Zᵗr           = Vector{T}(undef, q)\n",
    "    ZᵗΩ⁻¹r        = Vector{T}(undef, q)\n",
    "    LmmObs(y, X, Z, ∇β, ∇σ², ∇L, \n",
    "        yty, xty, zty, storage_p, storage_q, \n",
    "        xtx, ztx, ztz, storage_qq,\n",
    "        LM⁻¹LᵗZᵗZ, ZᵗZLM⁻¹LᵗZᵗr, Zᵗr, ZᵗΩ⁻¹r)\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    logl!(obs::LmmObs, β, L, σ², needgrad=false)\n",
    "\n",
    "Evaluate the log-likelihood of a single LMM datum at parameter values `β`, `L`, \n",
    "and `σ²`. If `needgrad == true`, then `obs.∇β`, `obs.∇L`, and `obs.σ²` are filled \n",
    "with the corresponding gradient.\n",
    "\"\"\"\n",
    "function logl!(\n",
    "        obs      :: LmmObs{T}, \n",
    "        β        :: Vector{T}, \n",
    "        L        :: Matrix{T}, \n",
    "        σ²       :: T,\n",
    "        needgrad :: Bool = true\n",
    "    ) where T <: AbstractFloat\n",
    "    n, p, q = size(obs.X, 1), size(obs.X, 2), size(obs.Z, 2)\n",
    "    ####################\n",
    "    # Evaluate objective\n",
    "    ####################    \n",
    "    # form the q-by-q matrix: M = σ² * I + Lᵗ Zᵗ Z L\n",
    "    copy!(obs.storage_qq, obs.ztz)\n",
    "    BLAS.trmm!('L', 'L', 'T', 'N', T(1), L, obs.storage_qq) # O(q^3)\n",
    "    BLAS.trmm!('R', 'L', 'N', 'N', T(1), L, obs.storage_qq) # O(q^3)\n",
    "    @inbounds for j in 1:q\n",
    "        obs.storage_qq[j, j] += σ²\n",
    "    end\n",
    "    # cholesky on M = σ² * I + Lᵗ Zᵗ Z L\n",
    "    LAPACK.potrf!('U', obs.storage_qq) # O(q^3)\n",
    "    # storage_q = (Mchol.U') \\ (Lt * (Zt * res))\n",
    "    BLAS.gemv!('N', T(-1), obs.ztx, β, T(1), copy!(obs.Zᵗr, obs.zty)) # O(pq)\n",
    "    BLAS.trmv!('L', 'T', 'N', L, copy!(obs.storage_q, obs.Zᵗr)) # O(q^2)\n",
    "    BLAS.trsv!('U', 'T', 'N', obs.storage_qq, obs.storage_q) # O(q^3)\n",
    "    # l2 norm of residual vector\n",
    "    copy!(obs.storage_p, obs.xty)\n",
    "    rtr  = obs.yty +\n",
    "        dot(β, BLAS.gemv!('N', T(1), obs.xtx, β, T(-2), obs.storage_p))\n",
    "    # assemble pieces\n",
    "    logl::T = n * log(2π) + (n - q) * log(σ²) # constant term\n",
    "    @inbounds for j in 1:q\n",
    "        logl += 2log(obs.storage_qq[j, j])\n",
    "    end\n",
    "    qf    = abs2(norm(obs.storage_q)) # quadratic form term\n",
    "    logl += (rtr - qf) / σ² \n",
    "    logl /= -2\n",
    "    ###################\n",
    "    # Evaluate gradient\n",
    "    ###################    \n",
    "    if needgrad\n",
    "        # TODO: fill ∇β, ∇σ², ∇L by gradients\n",
    "        # compute ∇β\n",
    "        copy!(obs.∇β, obs.xty) \n",
    "        BLAS.gemv!('N', T(-1), obs.xtx, β, T(1), obs.∇β)\n",
    "        BLAS.trsv!('U', 'N', 'N', obs.storage_qq, obs.storage_q)\n",
    "        BLAS.trmv!('L', 'N', 'N', L, obs.storage_q) # LM⁻¹LᵗZᵗr\n",
    "        BLAS.gemv!('T', T(-1), obs.ztx, obs.storage_q, T(1), obs.∇β)\n",
    "        obs.∇β ./= σ²\n",
    "        # compute ∇σ² \n",
    "        obs.∇σ²[1] = - n \n",
    "        BLAS.gemm!('T', 'N', T(1), L, obs.ztz, T(0), obs.LM⁻¹LᵗZᵗZ)\n",
    "        BLAS.trsm!('L', 'U', 'T', 'N', T(1), obs.storage_qq, obs.LM⁻¹LᵗZᵗZ)\n",
    "        BLAS.trsm!('L', 'U', 'N', 'N', T(1), obs.storage_qq, obs.LM⁻¹LᵗZᵗZ)\n",
    "        BLAS.trmm!('L', 'L', 'N', 'N', T(1), L, obs.LM⁻¹LᵗZᵗZ)\n",
    "        obs.∇σ²[1] += tr(obs.LM⁻¹LᵗZᵗZ)\n",
    "        BLAS.gemv!('N', T(1), obs.ztz, obs.storage_q, T(0), obs.ZᵗZLM⁻¹LᵗZᵗr)\n",
    "        obs.∇σ²[1] += (rtr - 2 * qf + dot(obs.storage_q, obs.ZᵗZLM⁻¹LᵗZᵗr)) / σ²\n",
    "        obs.∇σ²[1] /= (2 * σ²)\n",
    "        # compute ∇L\n",
    "        copy!(obs.∇L, obs.ztz)\n",
    "        BLAS.gemm!('N', 'N', T(1), obs.ztz, obs.LM⁻¹LᵗZᵗZ, T(-1), obs.∇L)\n",
    "        obs.∇L ./= σ²\n",
    "        obs.ZᵗΩ⁻¹r .= (obs.Zᵗr .- obs.ZᵗZLM⁻¹LᵗZᵗr) ./ σ²\n",
    "        BLAS.ger!(T(1), obs.ZᵗΩ⁻¹r, obs.ZᵗΩ⁻¹r, obs.∇L)\n",
    "        # L is multiplied later to reduce flops\n",
    "    end\n",
    "    ###################\n",
    "    # Return\n",
    "    ###################        \n",
    "    return logl    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a good idea to test correctness and efficiency of the single datum objective/gradient evaluator here. First generate the same data set as in HW2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(257)\n",
    "# dimension\n",
    "n, p, q = 2000, 5, 3\n",
    "# predictors\n",
    "X  = [ones(n) randn(n, p - 1)]\n",
    "Z  = [ones(n) randn(n, q - 1)]\n",
    "# parameter values\n",
    "β  = [2.0; -1.0; rand(p - 2)]\n",
    "σ² = 1.5\n",
    "Σ  = fill(0.1, q, q) + 0.9I # compound symmetry \n",
    "L  = Matrix(cholesky(Symmetric(Σ)).L)\n",
    "# generate y\n",
    "y  = X * β + Z * rand(MvNormal(Σ)) + sqrt(σ²) * randn(n)\n",
    "\n",
    "# form the LmmObs object\n",
    "obs = LmmObs(y, X, Z);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logl = logl!(obs, β, L, σ², true) = -3261.9177559187597\n",
      "obs.∇β = [-1.1937579038479573, -20.06810413006247, -12.323942329522415, -5.174641191562259, 28.20847569099026]\n",
      "obs.∇σ² = [5.50162054089166]\n",
      "obs.∇L = [0.4073285664341204 -0.10370457194287118 0.9214023976865059; -0.10370457194294816 -0.9907411137746662 -0.02165831118499488; 0.9214023976864656 -0.02165831118503278 -0.5355581080218288]\n"
     ]
    }
   ],
   "source": [
    "@show logl = logl!(obs, β, L, σ², true)\n",
    "@show obs.∇β\n",
    "@show obs.∇σ²\n",
    "@show obs.∇L;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will lose all 20 points if following statement throws `AssertionError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@assert abs(logl - (-3261.9177559187597)) < 1e-8\n",
    "@assert norm(obs.∇β - [-1.1937579038479573, -20.068104130062466, \n",
    "        -12.323942329522424, -5.174641191562253, 28.208475690990248]) < 1e-8\n",
    "@assert norm(obs.∇L - [0.40732856643442306 -0.10370457194285436 0.9214023976868783; \n",
    "        -0.10370457194285436 -0.9907411137746756 -0.02165831118509598; \n",
    "        0.9214023976868783 -0.02165831118509598 -0.5355581080215592]) < 1e-8\n",
    "@assert abs(obs.∇σ²[1] - (5.501620540891395)) < 1e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark for evaluating objective only. This is what we did in HW2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  0 bytes\n",
       "  allocs estimate:  0\n",
       "  --------------\n",
       "  minimum time:     617.486 ns (0.00% GC)\n",
       "  median time:      663.659 ns (0.00% GC)\n",
       "  mean time:        718.516 ns (0.00% GC)\n",
       "  maximum time:     2.280 μs (0.00% GC)\n",
       "  --------------\n",
       "  samples:          10000\n",
       "  evals/sample:     173"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@benchmark logl!($obs, $β, $L, $σ², false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark for objective + gradient evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  0 bytes\n",
       "  allocs estimate:  0\n",
       "  --------------\n",
       "  minimum time:     1.794 μs (0.00% GC)\n",
       "  median time:      1.917 μs (0.00% GC)\n",
       "  mean time:        2.259 μs (0.00% GC)\n",
       "  maximum time:     17.449 μs (0.00% GC)\n",
       "  --------------\n",
       "  samples:          10000\n",
       "  evals/sample:     10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm_objgrad = @benchmark logl!($obs, $β, $L, $σ², true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My median runt time is 2.1μs. You will get full credit (10 pts) if the median run time is within 10μs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  The points you will get are\n",
    "clamp(10 / (median(bm_objgrad).time / 1e3) * 10, 0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check for type stability\n",
    "# @code_warntype logl!(obs, β, L, σ², true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Profile\n",
    "# Profile.clear()\n",
    "# @profile for i in 1:10000; logl!(obs, β, L, σ², true); end\n",
    "# Profile.print(format=:flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. LmmModel type\n",
    "\n",
    "We create a `LmmModel` type to hold all data points and model parameters. Log-likelihood/gradient of a `LmmModel` object is simply the sum of log-likelihood/gradient of individual data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "logl!"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a type that holds LMM model (data + parameters)\n",
    "struct LmmModel{T <: AbstractFloat} <: MathProgBase.AbstractNLPEvaluator\n",
    "    # data\n",
    "    data :: Vector{LmmObs{T}}\n",
    "    # parameters\n",
    "    β    :: Vector{T}\n",
    "    L    :: Matrix{T}\n",
    "    σ²   :: Vector{T}    \n",
    "    # arrays for holding gradient\n",
    "    ∇β   :: Vector{T}\n",
    "    ∇σ²  :: Vector{T}\n",
    "    ∇L   :: Matrix{T}\n",
    "    # TODO: add whatever intermediate arrays you may want to pre-allocate\n",
    "    xty  :: Vector{T}\n",
    "    ztr2 :: Vector{T}\n",
    "    xtx  :: Matrix{T}\n",
    "    ztz2 :: Matrix{T}\n",
    "    storage_pp  :: Matrix{T}\n",
    "    storage_qq2 :: Matrix{T}\n",
    "    ztr         :: Vector{T}\n",
    "    storage_qq  :: Matrix{T}\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    LmmModel(data::Vector{LmmObs})\n",
    "\n",
    "Create an LMM model that contains data and parameters.\n",
    "\"\"\"\n",
    "function LmmModel(obsvec::Vector{LmmObs{T}}) where T <: AbstractFloat\n",
    "    # dims\n",
    "    p    = size(obsvec[1].X, 2)\n",
    "    q    = size(obsvec[1].Z, 2)\n",
    "    # parameters\n",
    "    β    = Vector{T}(undef, p)\n",
    "    L    = Matrix{T}(undef, q, q)\n",
    "    σ²   = Vector{T}(undef, 1)    \n",
    "    # gradients\n",
    "    ∇β   = similar(β)    \n",
    "    ∇σ²  = similar(σ²)\n",
    "    ∇L   = similar(L)\n",
    "    # intermediate arrays\n",
    "    xty  = Vector{T}(undef, p)\n",
    "    ztr2 = Vector{T}(undef, abs2(q))\n",
    "    xtx  = Matrix{T}(undef, p, p)\n",
    "    ztz2 = Matrix{T}(undef, abs2(q), abs2(q))\n",
    "    storage_pp = Matrix{T}(undef, p, p)\n",
    "    storage_qq2 = Matrix{T}(undef, abs2(q), abs2(q))\n",
    "    ztr         = Vector{T}(undef, q)\n",
    "    storage_qq  = Matrix{T}(undef, q, q)\n",
    "    LmmModel(obsvec, β, L, σ², ∇β, ∇σ², ∇L, xty, ztr2, xtx, ztz2,\n",
    "        storage_pp, storage_qq2, ztr, storage_qq)\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    logl!(m::LmmModel, needgrad=false)\n",
    "\n",
    "Evaluate the log-likelihood of an LMM model at parameter values `m.β`, `m.L`, \n",
    "and `m.σ²`. If `needgrad == true`, then `m.∇β`, `m.∇L`, and `m.σ²` are filled \n",
    "with the corresponding gradient.\n",
    "\"\"\"\n",
    "function logl!(m::LmmModel{T}, needgrad::Bool = false) where T <: AbstractFloat\n",
    "    logl = zero(T)\n",
    "    if needgrad\n",
    "        fill!(m.∇β , 0)\n",
    "        fill!(m.∇L , 0)\n",
    "        fill!(m.∇σ², 0)        \n",
    "    end\n",
    "    @inbounds for i in 1:length(m.data)\n",
    "        obs = m.data[i]\n",
    "        logl += logl!(obs, m.β, m.L, m.σ²[1], needgrad)\n",
    "        if needgrad\n",
    "            BLAS.axpy!(T(1), obs.∇β, m.∇β)\n",
    "            BLAS.axpy!(T(1), obs.∇L, m.∇L)\n",
    "            m.∇σ²[1] += obs.∇σ²[1]\n",
    "        end\n",
    "    end\n",
    "    # obtain gradient wrt L: m.∇L = m.∇L * L\n",
    "    if needgrad\n",
    "        BLAS.trmm!('R', 'L', 'N', 'N', T(1), m.L, m.∇L)\n",
    "    end\n",
    "    logl\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. (20 pts) Test data\n",
    "\n",
    "Let's generate a fake longitudinal data set to test our algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(257)\n",
    "\n",
    "# dimension\n",
    "m      = 1000 # number of individuals\n",
    "ns     = rand(1500:2000, m) # numbers of observations per individual\n",
    "p      = 5 # number of fixed effects, including intercept\n",
    "q      = 3 # number of random effects, including intercept\n",
    "obsvec = Vector{LmmObs{Float64}}(undef, m)\n",
    "# true parameter values\n",
    "βtrue  = [0.1; 6.5; -3.5; 1.0; 5]\n",
    "σ²true = 1.5\n",
    "σtrue  = sqrt(σ²true)\n",
    "Σtrue  = Matrix(Diagonal([2.0; 1.2; 1.0]))\n",
    "Ltrue  = Matrix(cholesky(Symmetric(Σtrue)).L)\n",
    "# generate data\n",
    "for i in 1:m\n",
    "    # first column intercept, remaining entries iid std normal\n",
    "    X = Matrix{Float64}(undef, ns[i], p)\n",
    "    X[:, 1] .= 1\n",
    "    @views Distributions.rand!(Normal(), X[:, 2:p])\n",
    "    # first column intercept, remaining entries iid std normal\n",
    "    Z = Matrix{Float64}(undef, ns[i], q)\n",
    "    Z[:, 1] .= 1\n",
    "    @views Distributions.rand!(Normal(), Z[:, 2:q])\n",
    "    # generate y\n",
    "    y = X * βtrue .+ Z * (Ltrue * randn(q)) .+ σtrue * randn(ns[i])\n",
    "    # form a LmmObs instance\n",
    "    obsvec[i] = LmmObs(y, X, Z)\n",
    "end\n",
    "# form a LmmModel instance\n",
    "lmm = LmmModel(obsvec);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later comparison with other software, we save the data into a text file `lmm_data.csv`. **Do not put this file in Git.** It takes 246.6MB storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "(isfile(\"lmm_data.csv\") && filesize(\"lmm_data.csv\") == 244697920) || \n",
    "open(\"lmm_data.csv\", \"w\") do io\n",
    "    p = size(lmm.data[1].X, 2)\n",
    "    q = size(lmm.data[1].Z, 2)\n",
    "    # print header\n",
    "    print(io, \"ID,Y,\")\n",
    "    for j in 1:(p - 1)\n",
    "        print(io, \"X\" * string(j) * \",\")\n",
    "    end\n",
    "    for j in 1:(q - 1)\n",
    "        print(io, \"Z\" * string(j) * (j < q - 1 ? \",\" : \"\\n\"))\n",
    "    end\n",
    "    # print data\n",
    "    for i in eachindex(lmm.data)\n",
    "        obs = lmm.data[i]\n",
    "        for j in 1:length(obs.y)\n",
    "            # id\n",
    "            print(io, i, \",\")\n",
    "            # Y\n",
    "            print(io, obs.y[j], \",\")\n",
    "            # X data\n",
    "            for k in 2:p\n",
    "                print(io, obs.X[j, k], \",\")\n",
    "            end\n",
    "            # Z data\n",
    "            for k in 2:(q - 1)\n",
    "                print(io, obs.Z[j, k], \",\")\n",
    "            end\n",
    "            print(io, obs.Z[j, q], \"\\n\")\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctness\n",
    "\n",
    "Evaluate log-likelihood and gradient of whole data set at the true parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obj = logl!(lmm, true) = -2.8342752296337807e6\n",
      "lmm.∇β = [11.772120067093992, -1925.6905435284602, -590.4811426628462, 2112.4417700624854, 2261.460483839501]\n",
      "lmm.∇σ² = [911.8747620552729]\n",
      "lmm.∇L = [24.41990131135614 24.011922772756076 16.56376848157203; 30.99925900310029 -59.40190266158887 46.58331413472227; 23.42470603064739 51.02946390987321 12.038870789780153]\n"
     ]
    }
   ],
   "source": [
    "copy!(lmm.β, βtrue)\n",
    "copy!(lmm.L, Ltrue)\n",
    "lmm.σ²[1] = σ²true\n",
    "@show obj = logl!(lmm, true)\n",
    "@show lmm.∇β\n",
    "@show lmm.∇σ²\n",
    "@show lmm.∇L;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test correctness. You will loss all 20 points if following code throws `AssertError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@assert abs(obj - (-2.8342752296337797e6)) < 1e-6\n",
    "@assert norm(lmm.∇β - [11.772120067073923, -1925.690543528459, \n",
    "        -590.4811426628461, 2112.441770062488, 2261.4604838395016]) < 1e-6\n",
    "@assert norm(lmm.∇L - [24.419901311377576 24.01192277273847 16.563768481574012; \n",
    "        30.99925900307824 -59.40190266157969 46.58331413470851; \n",
    "        23.424706030649975 51.029463909858194 12.038870789777398]) < 1e-6\n",
    "@assert abs(lmm.∇σ²[1] - (911.8747620554551)) < 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiency\n",
    "\n",
    "Test efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  0 bytes\n",
       "  allocs estimate:  0\n",
       "  --------------\n",
       "  minimum time:     1.765 ms (0.00% GC)\n",
       "  median time:      1.817 ms (0.00% GC)\n",
       "  mean time:        1.849 ms (0.00% GC)\n",
       "  maximum time:     3.571 ms (0.00% GC)\n",
       "  --------------\n",
       "  samples:          2700\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm_model = @benchmark logl!($lmm, true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My median run time is 2.122ms. You will get full credit if your median run time is within 10ms. The points you will get are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clamp(10 / (median(bm_model).time / 1e6) * 10, 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory\n",
    "\n",
    "You will lose 1 point for each 100 bytes memory allocation. So the points you will get is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clamp(10 - median(bm_model).memory / 100, 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. (30 pts) Starting point\n",
    "\n",
    "For numerical optimization, a good starting point is critical. Let's start $\\boldsymbol{\\beta}$ and $\\sigma^2$ from the least sqaures solutions (ignoring intra-individual correlations)\n",
    "\\begin{eqnarray*}\n",
    "\\boldsymbol{\\beta}^{(0)} &=& \\left(\\sum_i \\mathbf{X}_i^T \\mathbf{X}_i\\right)^{-1} \\left(\\sum_i \\mathbf{X}_i^T \\mathbf{y}_i\\right) \\\\\n",
    "\\sigma^{2(0)} &=& \\frac{\\sum_i \\|\\mathbf{r}_i^{(0)}\\|_2^2}{\\sum_i n_i} = \\frac{\\sum_i \\|\\mathbf{y}_i - \\mathbf{X}_i \\boldsymbol{\\beta}^{(0)}\\|_2^2}{\\sum_i n_i}.\n",
    "\\end{eqnarray*}\n",
    "To get a reasonable starting point for $\\boldsymbol{\\Sigma} = \\mathbf{L} \\mathbf{L}^T$, we can minimize the least squares criterion (ignoring the noise variance component)\n",
    "$$\n",
    "    \\text{minimize} \\sum_i \\| \\mathbf{r}_i^{(0)} \\mathbf{r}_i^{(0)T} - \\mathbf{Z}_i \\boldsymbol{\\Sigma} \\mathbf{Z}_i^T \\|_{\\text{F}}^2.\n",
    "$$\n",
    "Derive the minimizer $\\boldsymbol{\\Sigma}^{(0)}$ (10 pts). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $f = \\sum_i \\| \\mathbf{r}_i^{(0)} \\mathbf{r}_i^{(0)T} - \\mathbf{Z}_i \\boldsymbol{\\Sigma} \\mathbf{Z}_i^T \\|_{\\text{F}}^2 = \\text{tr}(\\sum_i (\\mathbf{r}_i^{(0)} \\mathbf{r}_i^{(0)T} - \\mathbf{Z}_i \\boldsymbol{\\Sigma} \\mathbf{Z}_i^T)^T (\\mathbf{r}_i^{(0)} \\mathbf{r}_i^{(0)T} - \\mathbf{Z}_i \\boldsymbol{\\Sigma} \\mathbf{Z}_i^T))$. Then"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{eqnarray*}\n",
    "\\text{d} f &=& \\text{tr} (\\sum_i - 2 \\mathbf{Z}_i^T (\\mathbf{r}_i^{(0)} \\mathbf{r}_i^{(0)T} - \\mathbf{Z}_i \\boldsymbol{\\Sigma} \\mathbf{Z}_i^T) \\mathbf{Z}_i \\text{d} \\boldsymbol{\\Sigma}) \\\\\n",
    "\\therefore \\frac{\\partial}{\\partial \\boldsymbol{\\Sigma}} f &=& \\sum_i - 2 \\mathbf{Z}_i^T (\\mathbf{r}_i^{(0)} \\mathbf{r}_i^{(0)T} - \\mathbf{Z}_i \\boldsymbol{\\Sigma} \\mathbf{Z}_i^T) \\mathbf{Z}_i \\\\\n",
    "&& \\rightarrow \\sum_i \\mathbf{Z}_i^T \\mathbf{Z}_i \\boldsymbol{\\Sigma}^{(0)} \\mathbf{Z}_i^T \\mathbf{Z}_i = \n",
    "\\sum_i \\mathbf{Z}_i^T \\mathbf{r}_i^{(0)} \\mathbf{r}_i^{(0)T} \\mathbf{Z}_i \\\\\n",
    "&& \\text{vec} (\\sum_i \\mathbf{Z}_i^T \\mathbf{Z}_i \\boldsymbol{\\Sigma}^{(0)} \\mathbf{Z}_i^T \\mathbf{Z}_i) = \n",
    "\\text{vec}(\\sum_i \\mathbf{Z}_i^T \\mathbf{r}_i^{(0)} \\mathbf{r}_i^{(0)T} \\mathbf{Z}_i) \\\\\n",
    "&& \\sum_i (\\mathbf{Z}_i^T \\mathbf{Z}_i \\otimes \\mathbf{Z}_i^T \\mathbf{Z}_i) \\text{vec} (\\boldsymbol{\\Sigma}^{(0)}) = \\sum_i (\\mathbf{Z}_i^T \\mathbf{r}_i^{(0)} \\otimes \\mathbf{Z}_i^T \\mathbf{r}_i^{(0)}) \\\\\n",
    "\\therefore \\text{vec} (\\boldsymbol{\\Sigma}^{(0)}) &=& [\\sum_i (\\mathbf{Z}_i^T \\mathbf{Z}_i \\otimes \\mathbf{Z}_i^T \\mathbf{Z}_i)]^{-1} [\\sum_i (\\mathbf{Z}_i^T \\mathbf{r}_i^{(0)} \\otimes \\mathbf{Z}_i^T \\mathbf{r}_i^{(0)})]\n",
    "\\end{eqnarray*} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement this start point strategy in the function `init_ls()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "init_ls!"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    init_ls!(m::LmmModel)\n",
    "\n",
    "Initialize parameters of a `LmmModel` object from the least squares estimate. \n",
    "`m.β`, `m.L`, and `m.σ²` are overwritten with the least squares estimates.\n",
    "\"\"\"\n",
    "function init_ls!(m::LmmModel{T}) where T <: AbstractFloat\n",
    "    p, q = size(m.data[1].X, 2), size(m.data[1].Z, 2)\n",
    "    # TODO: fill m.β, m.σ², m.L by LS estimates\n",
    "    # initialize arrays\n",
    "    n = T(0)\n",
    "    m.xtx .= T(0)\n",
    "    m.xty .= T(0)\n",
    "    m.σ²[1] = 0\n",
    "    @inbounds for i in 1:length(m.data)\n",
    "        obs = m.data[i]\n",
    "        n += size(obs.X, 1)\n",
    "        m.xtx .+= obs.xtx\n",
    "        m.xty .+= obs.xty\n",
    "        m.σ²[1] += obs.yty\n",
    "    end\n",
    "    # compute m.β\n",
    "    copy!(m.storage_pp, m.xtx)\n",
    "    copy!(m.β, m.xty)\n",
    "    LAPACK.potrf!('U', m.storage_pp)\n",
    "    LAPACK.potrs!('U', m.storage_pp, m.β)\n",
    "    # compute m.σ²\n",
    "    m.σ²[1] += dot(m.β, BLAS.gemv!('N', T(1), m.xtx, m.β, T(-2), m.xty))\n",
    "    m.σ²[1] /= n\n",
    "    # compute m.L\n",
    "    m.ztz2 .= T(0)\n",
    "    m.storage_qq .= T(0)\n",
    "    @inbounds for i in 1:length(m.data)\n",
    "        obs = m.data[i]\n",
    "        kron!(m.storage_qq2, obs.ztz, obs.ztz)\n",
    "        BLAS.axpy!(T(1), m.storage_qq2, m.ztz2)\n",
    "        copy!(m.ztr, obs.zty)\n",
    "        BLAS.gemv!('N', T(-1), obs.ztx, m.β, T(1), m.ztr)\n",
    "        BLAS.ger!(T(1), m.ztr, m.ztr, m.storage_qq)\n",
    "    end\n",
    "    m.ztr2 .= vec(m.storage_qq)\n",
    "    LAPACK.potrf!('U', m.ztz2)\n",
    "    LAPACK.potrs!('U', m.ztz2, m.ztr2)\n",
    "    m.L .= reshape(m.ztr2, q, q)\n",
    "    LAPACK.potrf!('L', m.L)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logl!(lmm) = -3.3529912271664576e6\n",
      "lmm.β = [0.1281424806084756, 6.497945035096697, -3.502138774770135, 1.0026609224782135, 5.002697971721905]\n",
      "lmm.σ² = [5.701709885959523]\n",
      "lmm.L = [1.442089600524507 0.05523492575567489 0.04021508338255686; 0.03830200684866266 1.0546394793870693 0.06543803114908457; 0.027886674564416868 0.061034995187494806 1.0014753586943541]\n"
     ]
    }
   ],
   "source": [
    "init_ls!(lmm)\n",
    "@show logl!(lmm)\n",
    "@show lmm.β\n",
    "@show lmm.σ²\n",
    "@show lmm.L;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctness\n",
    "\n",
    "Your start points should have a log-likelihood larger than -3.352991e6 (10 pts). The points you get are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the points you get\n",
    "(logl!(lmm) >  -3.353e6) * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiency\n",
    "\n",
    "The start point should be computed quickly. Otherwise there is no point using it as a starting point. You get full credit (10 pts) if the median run time is within 1ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  176 bytes\n",
       "  allocs estimate:  4\n",
       "  --------------\n",
       "  minimum time:     208.764 μs (0.00% GC)\n",
       "  median time:      217.270 μs (0.00% GC)\n",
       "  mean time:        223.764 μs (0.00% GC)\n",
       "  maximum time:     602.101 μs (0.00% GC)\n",
       "  --------------\n",
       "  samples:          10000\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm_init = @benchmark init_ls!($lmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the points you get\n",
    "clamp(1 / (median(bm_init).time / 1e6) * 10, 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6. NLP via MathProgBase.jl\n",
    "\n",
    "We define the NLP problem using the modelling tool MathProgBase.jl. Start-up code is given below. Modify if necessary to accomodate your own code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    fit!(m::LmmModel, solver = Ipopt.IpoptSolver(print_level = 5))\n",
    "\n",
    "Fit an `LmmModel` object by MLE using a nonlinear programming solver. Start point \n",
    "should be provided in `m.β`, `m.σ²`, `m.L`.\n",
    "\"\"\"\n",
    "function fit!(\n",
    "        m::LmmModel,\n",
    "        solver = Ipopt.IpoptSolver(print_level = 5)\n",
    "    )\n",
    "    p    = size(m.data[1].X, 2)\n",
    "    q    = size(m.data[1].Z, 2)\n",
    "    npar = p + ((q * (q + 1)) >> 1) + 1\n",
    "    optm = MathProgBase.NonlinearModel(solver)\n",
    "    # set lower bounds and upper bounds of parameters\n",
    "    # diagonal entries of Cholesky factor L should be >= 0\n",
    "    lb   = fill(-Inf, npar)\n",
    "    ub   = fill( Inf, npar)\n",
    "    offset = p + 1\n",
    "    for j in 1:q, i in j:q\n",
    "        i == j && (lb[offset] = 0)\n",
    "        offset += 1\n",
    "    end\n",
    "    # σ² should be >= 0\n",
    "    lb[end] = 0\n",
    "    MathProgBase.loadproblem!(optm, npar, 0, lb, ub, Float64[], Float64[], :Max, m)\n",
    "    # starting point\n",
    "    par0 = zeros(npar)\n",
    "    modelpar_to_optimpar!(par0, m)\n",
    "    MathProgBase.setwarmstart!(optm, par0)\n",
    "    # optimize\n",
    "    MathProgBase.optimize!(optm)\n",
    "    optstat = MathProgBase.status(optm)\n",
    "    optstat == :Optimal || @warn(\"Optimization unsuccesful; got $optstat\")\n",
    "    # update parameters and refresh gradient\n",
    "    optimpar_to_modelpar!(m, MathProgBase.getsolution(optm))\n",
    "    logl!(m, true)\n",
    "    m\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    modelpar_to_optimpar!(par, m)\n",
    "\n",
    "Translate model parameters in `m` to optimization variables in `par`.\n",
    "\"\"\"\n",
    "function modelpar_to_optimpar!(\n",
    "        par :: Vector,\n",
    "        m   :: LmmModel\n",
    "    )\n",
    "    p = size(m.data[1].X, 2)\n",
    "    q = size(m.data[1].Z, 2)\n",
    "    # β\n",
    "    copyto!(par, m.β)\n",
    "    # L\n",
    "    offset = p + 1\n",
    "    @inbounds for j in 1:q, i in j:q\n",
    "        par[offset] = m.L[i, j]\n",
    "        offset += 1\n",
    "    end\n",
    "    # σ²\n",
    "    par[end] = m.σ²[1]\n",
    "    par\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    optimpar_to_modelpar!(m, par)\n",
    "\n",
    "Translate optimization variables in `par` to the model parameters in `m`.\n",
    "\"\"\"\n",
    "function optimpar_to_modelpar!(\n",
    "        m   :: LmmModel, \n",
    "        par :: Vector\n",
    "    )\n",
    "    p = size(m.data[1].X, 2)\n",
    "    q = size(m.data[1].Z, 2)\n",
    "    # β\n",
    "    copyto!(m.β, 1, par, 1, p)\n",
    "    # L\n",
    "    fill!(m.L, 0)\n",
    "    offset = p + 1\n",
    "    @inbounds for j in 1:q, i in j:q\n",
    "        m.L[i, j] = par[offset]\n",
    "        offset   += 1\n",
    "    end\n",
    "    # σ²\n",
    "    m.σ²[1] = par[end]    \n",
    "    m\n",
    "end\n",
    "\n",
    "function MathProgBase.initialize(\n",
    "        m                  :: LmmModel, \n",
    "        requested_features :: Vector{Symbol}\n",
    "    )\n",
    "    for feat in requested_features\n",
    "        if !(feat in [:Grad])\n",
    "            error(\"Unsupported feature $feat\")\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "MathProgBase.features_available(m::LmmModel) = [:Grad]\n",
    "\n",
    "function MathProgBase.eval_f(\n",
    "        m   :: LmmModel, \n",
    "        par :: Vector\n",
    "    )\n",
    "    optimpar_to_modelpar!(m, par)\n",
    "    logl!(m, false) # don't need gradient here\n",
    "end\n",
    "\n",
    "function MathProgBase.eval_grad_f(\n",
    "        m    :: LmmModel, \n",
    "        grad :: Vector, \n",
    "        par  :: Vector\n",
    "    )\n",
    "    p = size(m.data[1].X, 2)\n",
    "    q = size(m.data[1].Z, 2)\n",
    "    optimpar_to_modelpar!(m, par) \n",
    "    obj = logl!(m, true)\n",
    "    # gradient wrt β\n",
    "    copyto!(grad, m.∇β)\n",
    "    # gradient wrt L\n",
    "    offset = p + 1\n",
    "    @inbounds for j in 1:q, i in j:q\n",
    "        grad[offset] = m.∇L[i, j]\n",
    "        offset += 1\n",
    "    end\n",
    "    # gradient with respect to σ²\n",
    "    grad[end] = m.∇σ²[1]\n",
    "    # return objective\n",
    "    obj\n",
    "end\n",
    "\n",
    "MathProgBase.eval_g(m::LmmModel, g, par) = nothing\n",
    "MathProgBase.jac_structure(m::LmmModel) = Int[], Int[]\n",
    "MathProgBase.eval_jac_g(m::LmmModel, J, par) = nothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7. (20 pts) Test drive\n",
    "\n",
    "Now we can run our NLP solver to compute the MLE. For grading purpose, we first use the `:LD_LBFGS` (limited-memory BFGS) algorithm in NLopt.jl here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective value at starting point: -3.3529912271664576e6\n",
      "\n",
      "  0.846694 seconds (1.41 M allocations: 77.663 MiB, 2.20% gc time, 82.83% compilation time)\n",
      "objective value at solution: -2.834264502363213e6\n",
      "\n",
      "solution values:\n",
      "lmm.β = [0.12386889603721703, 6.498337266205817, -3.5005130831538285, 1.001821941366638, 5.001953717754234]\n",
      "lmm.σ² = [1.5023520948943991]\n",
      "lmm.L * transpose(lmm.L) = [2.0685026437145684 0.053017368805560926 0.03271277214524364; 0.053017368805560926 1.1218137937601163 0.05601656427214081; 0.03271277214524364 0.05601656427214081 1.0120501850565866]\n",
      "gradient @ solution:\n",
      "lmm.∇β = [-0.012588368305703646, 0.022504964788772952, 0.033356548214975135, 0.018718182031133068, -0.0036046348391605143]\n",
      "lmm.∇σ² = [-0.010259020848025102]\n",
      "lmm.∇L = [0.0202890887444799 0.044878123309928006 0.002827388279535815; 0.059890364718108845 -0.013672271563402377 -0.019725544405109402; 0.0034260857332175172 -0.020555932336614 0.004478488243637489]\n",
      "sqrt(abs2(norm(lmm.∇β)) + abs2(norm(lmm.∇σ²) + abs2(norm(LowerTriangular(lmm.∇L))))) = 0.04861005677730496\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.04861005677730496"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize from least squares\n",
    "init_ls!(lmm)\n",
    "println(\"objective value at starting point: \", logl!(lmm)); println()\n",
    "\n",
    "@time fit!(lmm, NLopt.NLoptSolver(algorithm = :LD_LBFGS, \n",
    "        ftol_rel = 1e-12, ftol_abs = 1e-12, \n",
    "        xtol_rel = 1e-12, xtol_abs = 1e-12, \n",
    "        maxeval = 10000));\n",
    "\n",
    "println(\"objective value at solution: \", logl!(lmm)); println()\n",
    "println(\"solution values:\")\n",
    "@show lmm.β\n",
    "@show lmm.σ²\n",
    "@show lmm.L * transpose(lmm.L)\n",
    "println(\"gradient @ solution:\")\n",
    "@show lmm.∇β\n",
    "@show lmm.∇σ²\n",
    "@show lmm.∇L\n",
    "@show sqrt(abs2(norm(lmm.∇β)) + abs2(norm(lmm.∇σ²) + \n",
    "        abs2(norm(LowerTriangular(lmm.∇L)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctness\n",
    "\n",
    "You get 10 points if the following code does not throw `AssertError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objective at solution should be close enough to the optimal\n",
    "@assert logl!(lmm) > -2.83427e6\n",
    "# gradient at solution should be small enough\n",
    "@assert sqrt(abs2(norm(lmm.∇β)) + abs2(norm(lmm.∇σ²) + \n",
    "        abs2(norm(LowerTriangular(lmm.∇L))))) < 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficiency\n",
    "\n",
    "My median run time is 132.5ms. You get 10 points if your median time is within 1s (=1000ms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  12.20 KiB\n",
       "  allocs estimate:  259\n",
       "  --------------\n",
       "  minimum time:     117.076 ms (0.00% GC)\n",
       "  median time:      131.381 ms (0.00% GC)\n",
       "  mean time:        131.836 ms (0.00% GC)\n",
       "  maximum time:     150.239 ms (0.00% GC)\n",
       "  --------------\n",
       "  samples:          38\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm_bfgs = @benchmark fit!($lmm, $(NLopt.NLoptSolver(algorithm = :LD_LBFGS, \n",
    "        ftol_rel = 1e-12, ftol_abs = 1e-12, \n",
    "        xtol_rel = 1e-12, xtol_abs = 1e-12, \n",
    "        maxeval = 10000))) setup = (init_ls!(lmm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the points you get\n",
    "clamp(1 / (median(bm_bfgs).time / 1e9) * 10, 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8. (10 pts) Gradient free vs gradient-based methods\n",
    "\n",
    "Advantage of using a modelling tool such as MathProgBase.jl is that we can easily switch the backend solvers. For a research problem, we never know beforehand which solver works best. \n",
    "\n",
    "Try different solvers in the NLopt.jl and Ipopt.jl packages. Compare the results in terms run times (the shorter the better), objective values at solution (the larger the better), and gradients at solution (closer to 0 the better). Summarize what you find.\n",
    "\n",
    "See this [page](https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/) for the descriptions of algorithms in NLopt.\n",
    "\n",
    "Documentation for the Ipopt can be found [here](https://coin-or.github.io/Ipopt/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************************************************************************\n",
      "This program contains Ipopt, a library for large-scale nonlinear optimization.\n",
      " Ipopt is released as open source code under the Eclipse Public License (EPL).\n",
      "         For more information visit https://github.com/coin-or/Ipopt\n",
      "******************************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# vector of solvers to compare\n",
    "solvers = [\n",
    "    # NLopt: gradient-based algorithms\n",
    "    NLopt.NLoptSolver(algorithm = :LD_LBFGS, \n",
    "        ftol_rel = 1e-12, ftol_abs = 1e-12,\n",
    "        xtol_rel = 1e-12, xtol_abs = 1e-12,\n",
    "        maxeval=10000),\n",
    "    NLopt.NLoptSolver(algorithm = :LD_MMA, \n",
    "        ftol_rel = 1e-12, ftol_abs = 1e-12, \n",
    "        xtol_rel = 1e-12, xtol_abs = 1e-12, \n",
    "        maxeval=10000),\n",
    "    # NLopt: gradient-free algorithms\n",
    "    NLopt.NLoptSolver(algorithm = :LN_BOBYQA, \n",
    "        ftol_rel = 1e-12, ftol_abs = 1e-12, \n",
    "        xtol_rel = 1e-12, xtol_abs = 1e-12, \n",
    "        maxeval = 10000),\n",
    "    # Ipopt\n",
    "    Ipopt.IpoptSolver(print_level = 0)\n",
    "]\n",
    "# containers for results\n",
    "runtime = zeros(length(solvers))\n",
    "objvals = zeros(length(solvers))\n",
    "gradnrm = zeros(length(solvers))\n",
    "\n",
    "for (i, solver) in enumerate(solvers)\n",
    "    bm = @benchmark fit!($lmm, $solver) setup = (init_ls!(lmm))\n",
    "    runtime[i] = median(bm).time / 1e9\n",
    "    objvals[i] = logl!(lmm, true)\n",
    "    gradnrm[i] = sqrt(abs2(norm(lmm.∇β)) + abs2(norm(lmm.∇σ²) + \n",
    "        abs2(norm(LowerTriangular(lmm.∇L)))))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>Runtime</th><th>Objective</th><th>Gradnorm</th></tr><tr><th></th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>4 rows × 3 columns</p><tr><th>1</th><td>0.119882</td><td>-2.83426e6</td><td>0.0486101</td></tr><tr><th>2</th><td>0.12911</td><td>-2.83426e6</td><td>0.00527842</td></tr><tr><th>3</th><td>0.0460048</td><td>-2.83539e6</td><td>31729.5</td></tr><tr><th>4</th><td>4.73216</td><td>-2.83426e6</td><td>6.73208e-6</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccc}\n",
       "\t& Runtime & Objective & Gradnorm\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & 0.119882 & -2.83426e6 & 0.0486101 \\\\\n",
       "\t2 & 0.12911 & -2.83426e6 & 0.00527842 \\\\\n",
       "\t3 & 0.0460048 & -2.83539e6 & 31729.5 \\\\\n",
       "\t4 & 4.73216 & -2.83426e6 & 6.73208e-6 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m4×3 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Runtime   \u001b[0m\u001b[1m Objective  \u001b[0m\u001b[1m Gradnorm       \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Float64   \u001b[0m\u001b[90m Float64    \u001b[0m\u001b[90m Float64        \u001b[0m\n",
       "─────┼───────────────────────────────────────\n",
       "   1 │ 0.119882   -2.83426e6      0.0486101\n",
       "   2 │ 0.12911    -2.83426e6      0.00527842\n",
       "   3 │ 0.0460048  -2.83539e6  31729.5\n",
       "   4 │ 4.73216    -2.83426e6      6.73208e-6"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataFrame(Runtime = runtime, Objective = objvals, Gradnorm = gradnrm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective values (i.e. log-likelihood) were more or less the same across four different solvers with `:LN_BOBYQA` slightly smaller than the others. Meanwhile `:LN_BOBYQA` had the shortest run time, but also had the largest gradient norm, suggesting that there is some tradeoff here between performance and accuracy of the solution. This is also the case with `IpoptSolver`, which has the longest run time but the smallest graident norm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9. (10 pts) Compare with existing art\n",
    "\n",
    "Let's compare our method with lme4 package in R and MixedModels.jl package in Julia. Both lme4 and MixedModels.jl are developed mainly by Doug Bates. Summarize what you find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "method  = [\"257\", \"lme4\", \"MixedModels.jl\"]\n",
    "runtime = zeros(3)  # record the run times\n",
    "loglike = zeros(3); # record the log-likelihood at MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.834264502360533e6"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm_257 = @benchmark fit!($lmm, $(NLopt.NLoptSolver(algorithm = :LD_MMA, \n",
    "        ftol_rel = 1e-12, ftol_abs = 1e-12, \n",
    "        xtol_rel = 1e-12, xtol_abs = 1e-12, \n",
    "        maxeval = 10000))) setup = (init_ls!(lmm))\n",
    "runtime[1] = (median(bm_257).time) / 1e9\n",
    "loglike[1] = logl!(lmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lme4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: RCall.jl: Warning: package ‘lme4’ was built under R version 4.0.2\n",
      "│ Loading required package: Matrix\n",
      "│ Warning: package ‘Matrix’ was built under R version 4.0.2\n",
      "└ @ RCall /Users/minsookim/.julia/packages/RCall/eRsxl/src/io.jl:160\n",
      "┌ Warning: RCall.jl: Warning: package ‘readr’ was built under R version 4.0.2\n",
      "└ @ RCall /Users/minsookim/.julia/packages/RCall/eRsxl/src/io.jl:160\n",
      "┌ Warning: RCall.jl: Warning: package ‘magrittr’ was built under R version 4.0.2\n",
      "└ @ RCall /Users/minsookim/.julia/packages/RCall/eRsxl/src/io.jl:160\n",
      "┌ Warning: RCall.jl: \n",
      "│ ── Column specification ────────────────────────────────────────────────────────\n",
      "│ cols(\n",
      "│   ID = col_double(),\n",
      "│   Y = col_double(),\n",
      "│   X1 = col_double(),\n",
      "│   X2 = col_double(),\n",
      "│   X3 = col_double(),\n",
      "│   X4 = col_double(),\n",
      "│   Z1 = col_double(),\n",
      "│   Z2 = col_double()\n",
      "│ )\n",
      "│ \n",
      "└ @ RCall /Users/minsookim/.julia/packages/RCall/eRsxl/src/io.jl:160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RObject{VecSxp}\n",
       "# A tibble: 1,740,119 x 8\n",
       "      ID       Y      X1     X2      X3      X4      Z1      Z2\n",
       "   <dbl>   <dbl>   <dbl>  <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n",
       " 1     1   2.26  -0.358   0.379 -0.107   0.784  -1.28   -1.96  \n",
       " 2     1  10.5   -0.592  -0.540  1.14    1.46   -0.747   1.26  \n",
       " 3     1 -13.6   -1.91    0.719  0.758  -0.865  -1.70    0.0890\n",
       " 4     1   5.30  -0.106  -0.170  0.279   0.742  -0.0259  0.827 \n",
       " 5     1  11.0   -0.0209 -1.80   1.72    0.761  -0.562  -1.47  \n",
       " 6     1  -0.313  0.565   1.99  -0.437  -0.102   1.27    1.38  \n",
       " 7     1  -0.688 -0.451   0.906  2.23   -0.386  -1.83    0.276 \n",
       " 8     1  -4.50  -1.38   -0.527 -0.0893  0.210   1.19    1.49  \n",
       " 9     1   8.48   1.20    0.148  1.19    0.172   2.22    1.11  \n",
       "10     1  -4.39  -0.435   1.85  -0.146  -0.0681 -1.16    1.03  \n",
       "# … with 1,740,109 more rows\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R\"\"\"\n",
    "library(lme4)\n",
    "library(readr)\n",
    "library(magrittr)\n",
    "\n",
    "testdata <- read_csv(\"lmm_data.csv\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RObject{RealSxp}\n",
       "   user  system elapsed \n",
       " 90.551   5.107  95.715 \n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R\"\"\"\n",
    "rtime <- system.time(mmod <- \n",
    "  lmer(Y ~ X1 + X2 + X3 + X4 + (1 + Z1 + Z2 | ID), testdata, REML = FALSE))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "R\"\"\"\n",
    "rtime <- rtime[\"elapsed\"]\n",
    "summary(mmod)\n",
    "rlogl <- logLik(mmod)\n",
    "\"\"\"\n",
    "runtime[2] = @rget rtime\n",
    "loglike[2] = @rget rlogl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MixedModels.jl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata = CSV.File(\"lmm_data.csv\", types = Dict(1 => String)) |> DataFrame!;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.828005883"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mj = fit(MixedModel, @formula(Y ~ X1 + X2 + X3 + X4 + (1 + Z1 + Z2 | ID)), testdata)\n",
    "bm_mm = @benchmark fit(MixedModel, @formula(Y ~ X1 + X2 + X3 + X4 + (1 + Z1 + Z2 | ID)), testdata)\n",
    "loglike[3] = loglikelihood(mj)\n",
    "runtime[3] = median(bm_mm).time / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  879.31 MiB\n",
       "  allocs estimate:  6110\n",
       "  --------------\n",
       "  minimum time:     786.158 ms (3.02% GC)\n",
       "  median time:      828.006 ms (8.80% GC)\n",
       "  mean time:        854.870 ms (9.97% GC)\n",
       "  maximum time:     993.851 ms (21.01% GC)\n",
       "  --------------\n",
       "  samples:          6\n",
       "  evals/sample:     1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "\\begin{tabular}\n",
       "{l | r | r | r | r | r}\n",
       " & Est. & SE & z & p & $\\sigma_\\text{ID}$ \\\\\n",
       "\\hline\n",
       "(Intercept) & 0.1238 & 0.0455 & 2.72 & 0.0064 & 1.4383 \\\\\n",
       "X1 & 6.4983 & 0.0009 & 6989.59 & <1e-99 &   \\\\\n",
       "X2 & -3.5005 & 0.0009 & -3765.89 & <1e-99 &   \\\\\n",
       "X3 & 1.0018 & 0.0009 & 1077.25 & <1e-99 &   \\\\\n",
       "X4 & 5.0020 & 0.0009 & 5377.18 & <1e-99 &   \\\\\n",
       "Z2 &  &  &  &  & 1.0060 \\\\\n",
       "Z1 &  &  &  &  & 1.0592 \\\\\n",
       "Residual & 1.2257 &  &  &  &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "|             |    Est. |     SE |        z |      p |   σ_ID |\n",
       "|:----------- | -------:| ------:| --------:| ------:| ------:|\n",
       "| (Intercept) |  0.1238 | 0.0455 |     2.72 | 0.0064 | 1.4383 |\n",
       "| X1          |  6.4983 | 0.0009 |  6989.59 | <1e-99 |        |\n",
       "| X2          | -3.5005 | 0.0009 | -3765.89 | <1e-99 |        |\n",
       "| X3          |  1.0018 | 0.0009 |  1077.25 | <1e-99 |        |\n",
       "| X4          |  5.0020 | 0.0009 |  5377.18 | <1e-99 |        |\n",
       "| Z2          |         |        |          |        | 1.0060 |\n",
       "| Z1          |         |        |          |        | 1.0592 |\n",
       "| Residual    |  1.2257 |        |          |        |        |\n"
      ],
      "text/plain": [
       "Linear mixed model fit by maximum likelihood\n",
       " Y ~ 1 + X1 + X2 + X3 + X4 + (1 + Z1 + Z2 | ID)\n",
       "     logLik     -2 logLik        AIC           AICc          BIC      \n",
       " -2834264.5024  5668529.0047  5668553.0047  5668553.0049  5668701.4383\n",
       "\n",
       "Variance components:\n",
       "            Column   VarianceStd.Dev.  Corr.\n",
       "ID       (Intercept)  2.06882 1.43834\n",
       "         Z1           1.12190 1.05920 +0.03\n",
       "         Z2           1.01213 1.00605 +0.02 +0.05\n",
       "Residual              1.50235 1.22570\n",
       " Number of obs: 1740119; levels of grouping factors: 1000\n",
       "\n",
       "  Fixed-effects parameters:\n",
       "───────────────────────────────────────────────────────\n",
       "                 Coef.   Std. Error         z  Pr(>|z|)\n",
       "───────────────────────────────────────────────────────\n",
       "(Intercept)   0.123844  0.0454563        2.72    0.0064\n",
       "X1            6.49834   0.000929716   6989.59    <1e-99\n",
       "X2           -3.50051   0.000929532  -3765.89    <1e-99\n",
       "X3            1.00182   0.00092998    1077.25    <1e-99\n",
       "X4            5.00195   0.000930219   5377.18    <1e-99\n",
       "───────────────────────────────────────────────────────"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(bm_mm)\n",
    "mj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>method</th><th>runtime</th><th>logl</th></tr><tr><th></th><th>String</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>3 rows × 3 columns</p><tr><th>1</th><td>257</td><td>0.131099</td><td>-2.83426e6</td></tr><tr><th>2</th><td>lme4</td><td>95.715</td><td>-2.83426e6</td></tr><tr><th>3</th><td>MixedModels.jl</td><td>0.828006</td><td>-2.83426e6</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccc}\n",
       "\t& method & runtime & logl\\\\\n",
       "\t\\hline\n",
       "\t& String & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & 257 & 0.131099 & -2.83426e6 \\\\\n",
       "\t2 & lme4 & 95.715 & -2.83426e6 \\\\\n",
       "\t3 & MixedModels.jl & 0.828006 & -2.83426e6 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m3×3 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m method         \u001b[0m\u001b[1m runtime   \u001b[0m\u001b[1m logl       \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m String         \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64    \u001b[0m\n",
       "─────┼───────────────────────────────────────\n",
       "   1 │ 257              0.131099  -2.83426e6\n",
       "   2 │ lme4            95.715     -2.83426e6\n",
       "   3 │ MixedModels.jl   0.828006  -2.83426e6"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataFrame(method = method, runtime = runtime, logl = loglike)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, `:LD_MMA` algorithm showed the shortest runtime, while the objective values were more or less the same across different methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q10. Be proud of yourself\n",
    "\n",
    "Go to your resume/cv and claim you have experience performing analysis on complex longitudinal data sets with millions of records. And you beat current software by ~750 fold. "
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "87px",
    "width": "252px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.188px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
